# Compétence 1 – Réaliser un développement d’application

## Contexte rapide

Au cours de ma **deuxième** année d'**alternance** au sein de l'entreprise Amphenol FCI Besançon j'ai été amené à
poursuivre le développement de l'application que j'ai pu réaliser en première année
afin **d'ajouter des fonctionnalités**. L'application est une application de gestion de feuille process, c'est-à-dire
que l'on peut y réaliser des relevés de caractéristiques sur des produits en production.
L'application est développée en **VueJS** côté frontend et **NodeJS** côté backend avec une base de données **PostgreSQL**.

L'application, pour rappel, permet de réaliser des **relevés** de caractéristique sur des produits en production afin
d'être conforme aux spécifications.
Un opérateur va donc, pour un produit, sur une ligne de production, relever toutes les caractéristiques étape par
étape et compléter le relevé.

Une fois le relevé réalisé celui-ci est enregistré dans la base de données permettant ainsi d'agir comme un historique
des relevés.

Au sein de ces historiques il y a donc toutes les valeurs relevées par l'opérateur à un moment `T`, des valeurs qui
peuvent
sortir des spécifications notamment.

Avec plusieurs relevés effectués quotidiennement (minimum 3 par jour), l'entreprise totalise actuellement plus de 1 500
relevés stockés en base de données. Ces données servent à la fois à l'historisation et au suivi de la qualité des
produits.

## Problématique identifiée

Le principal problème réside dans l'**impossibilité de visualiser rapidement et efficacement** l'évolution des
caractéristiques mesurées. Pour analyser l'évolution d'une caractéristique, il est nécessaire de consulter manuellement
chaque relevé individuellement, ce qui s'avère chronophage et peu pratique.

Cette limitation empêche de :

- Vérifier en temps réel si la production respecte les spécifications
- Détecter les dérives de caractéristiques avant qu'elles ne deviennent problématiques
- Prendre des mesures correctives préventives

## Impact métier

Cette problématique a récemment causé un **incident** qualité lors d'un audit client. Plusieurs relevés présentaient des
valeurs hors spécification qui n'ont pas pu être détectées à temps, faute d'outil de visualisation approprié. Cette
situation rappelle les difficultés rencontrées avant la numérisation des relevés.

<h1 id="Solution">Solution</h1>

Pour répondre à cette problématique, j'ai **conçu et développé un dashboard** de suivi des caractéristiques permettant
de visualiser l'évolution des données de production de manière graphique et intuitive.

<figure>
  <img alt="Dashboard" height="900" src="/portfolio/images_portfolio/dashboard.png" width="100%"/>
  <figcaption style="font-size: 1.2em;">
    Trace 1 : Tableau de bord de suivi des relevés 
  </figcaption>
</figure>

## Architecture de sécurité mise en place

L'accès au dashboard est sécurisé par un système d'authentification et d'autorisation basé sur les rôles utilisateurs :

### Gestion des accès par rôles

- **Administrateur** : Accès complet aux fonctionnalités
- **Technicien Process (TP)** : Consultation et gestion des feuilles process
- **Ingénieurs/Service Qualité** : Consultation des données statistiques

Les opérateurs de production n'ont volontairement pas accès à cette interface, leur rôle se limitant à la saisie des
relevés.

### Implémentation technique de la sécurité

J'ai mis en place une architecture de sécurité comprenant :

- **Hachage des mots de passe** avec bcrypt pour la protection des données d'authentification
- **Authentification** via JSON Web Token (JWT)
- **Vérification des autorisations** à chaque requête pour contrôler l'accès aux fonctionnalités selon le rôle
  utilisateur
- **Stockage sécurisé** des credentials en base de données

Cette approche garantit la confidentialité des données de production tout en permettant aux équipes autorisées d'accéder
aux outils d'analyse nécessaires.

Ces mesures de sécurité sont réutilisées dans les routes API du dashboard notamment. Cela, grâce à un middleware, (
Concept
qui permet d'appliquer une logique avant la récupération des données via l'endpoint API) qui va vérifier si
l'utilisateur est authentifié et autorisé à accéder à la ressource demandée.

Les middleware sont créés dans un dossier dédié, ce qui permet de les réutiliser facilement dans différentes routes API.
Par exemple, le middleware `auth.js` vérifie le token JWT et le rôle de l'utilisateur avant d'autoriser l'accès aux
données.

----------------

## Interface utilisateur du dashboard

Une fois authentifié, l'utilisateur accède au dashboard composé de plusieurs sections :

### Tableau de bord statistiques (Dashboard)

Une barre d'indicateurs, située au-dessus présente les métriques clés de production (voir [Trace 1](#Solution)) :

- **Taux de non-conformité** : Pourcentage de relevés présentant des caractéristiques hors spécifications
- **Volume de production** : Nombre de relevés effectués (mensuel, hebdomadaire, par ligne)
- **Indicateurs de performance** par ligne de production

*Exemple* : Une température avec des bornes de 60° à 80° déclenchera un statut "Hors Spécification" (HS) si la valeur
relevée atteint 90° lors de la complétion d'un relevé réalisé par l'opérateur. Un taux HS faible indique une production
conforme.

### Implémentation technique

#### Chargement des données

J'utilise le **lifecycle hook `onMounted`** de Vue.js pour déclencher la récupération des données dès
l'initialisation du composant :

- **Frontend** : Requêtes API via Axios
- **Backend** : Requêtes SQL
- **Traitement** : Calcul automatique des ratios de non-conformité

#### Système de classification automatique

Chaque relevé est automatiquement évalué lors de sa sauvegarde :

**Critères d'évaluation :**

- Valeurs hors spécifications techniques
- Données manquantes obligatoires

**Résultat :** Attribution d'un booléen `is_hors_spec` permettant le calcul en temps réel des indicateurs de qualité.

Cette approche garantit une traçabilité complète et des statistiques fiables pour le pilotage de la production.

---------------------

## Interface de sélection des données

Le dashboard propose un système de filtrage composé de quatre sélecteurs principaux visibles sur le panneau de gauche (
voir [Trace 1](#Solution)) :

- **Ligne de production** : Filtrage par ligne active uniquement
- **Référence produit** : Nom du produit à analyser
- **Caractéristique** : Type de mesure (température, intensité, tension, pH, débit, longueur de cellule)
- **Étape** : Phase spécifique du processus de production

### Gestion intelligente des données disponibles

J'ai implémenté un système de filtrage dynamique pour optimiser l'expérience utilisateur. Les étapes proposées sont
automatiquement filtrées selon la disponibilité des caractéristiques sélectionnées.

**Logique métier :** Chaque caractéristique n'est pas mesurée à toutes les étapes. Par exemple, une température peut
être relevée aux étapes 1 et 3 mais pas à l'étape 2. Le système vérifie donc la présence de données avant de proposer
les options. Par exemple il ne va pas y avoir de température à relever pour l'étape 8 qui concerne la caméra.

### Architecture de base de données

La complexité des relations entre étapes et caractéristiques est gérée par un modèle relationnel adapté :

**Structure des données :**

- Chaque étape possède un numéro, une description et un état
- Les caractéristiques sont liées aux étapes via des tables de jonction (`multi_temp`, `multi_ampere`, etc.)
- Chaque caractéristique définit des coefficients minimal et maximal

## Implémentation technique du filtrage

Afin donc de récupérer les étapes disponibles sur une caractéristique, je vais écrire la requête SQL.

Cette requête permet de récupérer les étapes disponibles pour une caractéristique donnée, à savoir celle choisie
dans le sélecteur, en fonction de la référence produit sélectionnée.

Côté backend cela se fait avec sequelize, un ORM (Mapping Objet-Relationnel :  il s'agit d'une technique utilisée pour
gérer les interactions entre les données d'une base de données relationnelle et les objets utilisés par une application)
pour Node.js. C'est une requête SQL dynamique, c'est à dire à
conditionnelle, où je vais vérifier quelle caractéristique est sélectionnée et ainsi adapter la requête en conséquence.

Les requêtes se trouvent au sein de fichiers services qui sont ensuite appelés par le contrôleur. Je peux ainsi créer un
endpoint API (ex: `/api/etapes`) qui va appeler cette fonction.
Cette requête permet de récupérer les étapes disponibles pour une caractéristique donnée, en fonction de la
référence produit sélectionnée.

Cette disposition des fichiers permet de structurer le code de manière modulaire et maintenable, facilitant ainsi les
évolutions futures.

Cela se passe côté backend, côté frontend j'utilise Axios pour interroger l'API et récupérer les données nécessaires à
l'affichage des sélecteurs.

L'architecture côté frontend se compose comme ceci :

- **services** : pour les appels API
- **stores** : pour la gestion de l'état avec Pinia

Globalement, j'ai mes fonctions de récupération de données dans le dossier `services`, et cela est ensuite géré dans le
dossier `stores` avec Pinia où j'appelle mes fonctions service. C'est une architecture simple mais efficace pour séparer
la logique métier de l'interface utilisateur.

Et ainsi je peux réutiliser ces fonctions dans différents composants Vue.js.

Cette approche permet d'éviter les sélections inutiles et garantit que l'utilisateur ne se retrouve pas avec des
graphiques vides.

## Maintenabilité et évolutivité de l'architecture

L'architecture que j'ai mise en place facilite l'ajout de nouvelles fonctionnalités et la maintenance du code existant :

**Côté backend :**

- **Séparation des responsabilités** : Les modèles Sequelize sont définis dans des fichiers dédiés, initialisés avec
  leurs relations dans un fichier central. Cette organisation permet d'ajouter facilement de nouvelles tables ou de
  modifier les relations existantes sans impacter le reste du code.
- **Isolation de la logique métier** : Les requêtes et traitements sont encapsulés dans des services, ce qui facilite la
  réutilisation. Pour ajouter une nouvelle fonctionnalité, il suffit de créer un nouveau service ou d'étendre un service
  existant.
- **Extensibilité des API** : La structure contrôleurs/routes permet d'ajouter rapidement de nouveaux endpoints sans
  modifier le code existant. J'ai pu ainsi ajouter toutes les fonctionnalités du dashboard sans perturber l'application
  de saisie des relevés.
- **Middlewares configurables** : Cette approche me permet d'insérer des traitements (validation, authentification) à
  n'importe quel point du flux de requêtes.

**Côté frontend :**

- **Abstraction des appels API** : L'isolation des appels dans des services permet de modifier la logique d'accès aux
  données sans impacter les composants d'interface.
- **Gestion centralisée de l'état** : Pinia permet de partager l'état entre composants sans créer de dépendances
  directes, facilitant ainsi l'ajout de nouvelles vues utilisant les mêmes données.
- **Composants réutilisables** : J'ai conçu mes composants Vue.js pour être indépendants et paramétrables, ce qui me
  permet de les réutiliser dans différentes parties de l'application.

J'utilise Pinia pour la gestion des states et aussi pour les requêtes API, le mieux serait peut être d'utiliser une
librairie comme tanstack query, c'est une librairie qui permet de gérer les requêtes API et le cache de manière
efficace, ça me permettrait de séparer davantage les appels API de la logique métier. Cependant pour l'instant ce n'est
pas un besoin primordial mais c'est une piste d'amélioration pour le futur.

---------------

Je vous ai parlé des **quatre sélecteurs** mais il en manque un, c'est plutôt deux sélecteurs en un.

L’utilisateur peut sélectionner un produit via un champ de saisie où il peut renseigner le nom du produit et
cela retournera en temps réel les produits correspondant au produit écrit, visible sur le panneau de gauche (
voir [Trace 1](#Solution)). Cela fonctionne avec la propriété LIKE en SQL où je récupère via v-model et donc une liaison
bi directionnelle le nom du produit.

Ainsi si par exemple j'écris dans le champ input **« F_77310-11 »** cela va retourner tout les produits commençant
par **« F_77310-11 »**. La recherche est en temps réel, ce qui pourrait causer d'éventuels problèmes si l'
application
était accessible à un grand nombre d'utilisateur et en simultané du fait qu'à chaque input il y a une requête effectuée
vers le serveur. Cependant il n'y a que une dizaine de personnes ayant accès au Dashboard et en simultané il y a au
mieux
deux personnes.

Si l’optimisation devenait nécessaire, je pourrais implémenter une solution à savoir le **«
Debouncing »** qui est d'ajouter un temps d'arrêt avant que la requêtes ne s'éxécute, un temps de chargement finalement
pour brider les requêtes envoyées et in fine éviter la surcharge.

Une fois le produit entré, l'utilisateur peut, si il le souhaite sélectionner ces produits. Par défaut si l'utilisateur
clique sur **« Générer les statistiques »** cela prendra tout les produits commençant par, ici **« F_77310-11 »**
dans la
génération du graphique, si il sélectionne ces produits alors, je stocke ces derniers dans un tableau, en vérifiant si
il
n'existe pas déjà, si la taille du tableau est supérieure à 0 alors je prends les éléments de ce celui-ci pour la
génération du graphique.

L'utilisateur peut donc sélectionner un produit mais aussi le retirer, si il le souhaite, en cliquant dessus pour le
supprimer du tableau.

----------

Une fois le ou les produits sélectionné(s), je réalise alors deux requêtes

La première requête va me récupérer les valeurs mesurées de la caractéristique choisie avec ces paramètres :

- caracteristic,
- caracteristicOpti,
- nomLigne,
- pnPlating,
- etape
- mode

Le mode est simplement un chiffre pour me permettre de sélectionner le type de données envoyées pour le produit
sélectionné.

Si le mode est à 0 dans l'envoi de la requête l'on a affaire au tableau des valeurs sélectionnées, sinon l'on prend le
LIKE pour la requête. J'ai mis en place ce système, de base, pour une autre fonctionnalité concernant la mise à jour des
feuille process, où j'avais du code qui se répétait, j'ai donc réfléchi à une solution me permettant de segmenter le
type
de la requête, à savoir si je m'occupe d'une grande sélection `LIKE`, d'une sélection précise `=` ou d'une sélection
multiple `IN` à partir de là, avec Sequelize, je peux déterminer le type de requête.

| Mode | Type de requête                                     |
|------|-----------------------------------------------------|
| 0    | LIKE (pour les sélections multiples commençant par) |
| 1    | = (pour les sélections uniques)                     |
| 2    | IN (pour les sélections multiples exactes)          |

Une fois la sélection du mode faite, il me faut réaliser la requête, alors, c'est un peu particulier.
Un relevé fait par l'opérateur est stocké en base de donnée.

Un relevé est composé de plusieurs éléments :

- input_toggle (Concerne les cases à cocher pour déterminer si un état d'une étape est conforme)
- initiale_user (Les initiales de l'opérateur deux premières lettres du nom et du prénom exemple LALU pour Lauret Lucas)
- initiale_userat (Initiale de l'assistant technique qui aide pendant le relevé et le réglage de la ligne de production)
- initiale_usertp (Initiale du technicien process qui est null par défaut, se rempli si il vient ajouter un commentaire
  depuis son interface afin de compléter un relévé ou autre)
- commentaire_tp
- date_releve
- nom_pn_plating (nom du produit)
- commentaire (commentaire de l'opérateur sur le relevé pour justifier des valeurs HS)
- nom_ligne (Ligne de production)
- pn_brut (nom brut du produit)
- nom_famille (nom de la famille de produit)
- input_vitesse (objet json regroupant la vitesse relevée et ses coef)
- input_pression
- is_hors_spec (booléen)
- input_releve (Contient la totalité du relevé)

Les valeurs renseignées sont mises sous forme d'un objet JSON comme suit

```json
{
  "nom_etat": "2 FACES",
  "nom_ligne": "F",
  "pn_plating": "77391-122LF",
  "description": "Regulation moteur",
  "etape": "1",
  "station": "Degraissage1",
  "ph": {
    "phMax": 0,
    "phMin": 0,
    "phOpti": null
  },
  "debit": {
    "debitMax": 0,
    "debitMin": 0,
    "debitOpti": null
  },
  "voltage": {
    "voltageMax": 0,
    "voltageMin": 0,
    "voltageOpti": null
  },
  "amperage": {
    "amperageMax": 0,
    "amperageMin": 0,
    "amperageOpti": null
  },
  "temperature": {
    "temperatureMax": 0,
    "temperatureMin": 0,
    "temperatureOpti": null
  },
  "longueurCellule": {
    "longueurCelluleMax": 0,
    "longueurCelluleMin": 0,
    "longueurCelluleOpti": null
  }
}
...
  ```

Cet objet fois le nombre d'étape. Ce qui donne en moyenne une cinquantaine d'objets comme celui ci.

Cet objet est stocké directement dans la base dans une table appelée `historique`, j'ai décidé de stocker directement
l'objet JSON, simplement car cela aurait été plus complexe d'un point de vue relationnel au vu de la nature même d'une
feuille process, il aurait fallu créer soit plusieurs colonnes, soit le stocker en texte.
Mais j'ai décidé que ces `input_releve` soit stocké en JSON, en JSONB plus précisément.
Le type JSON va stocker en texte, tandis que le JSONB va stocker en format binaire la structure, c'est plus efficace
pour les requêtes.

Les requêtes postgreSQL sont différentes, il faut utiliser des flèches pour accéder aux objets et sous objets qui
m'intéresse, ici ce sont les valeurs dont la clé est "Opti" qui concerne les valeurs relevés, par défaut elles sont à
null, c'est à dire qu'il n'y a pas par exemple de température à cette étape d'où les coefficients à 0.
Visuellement pour l'opérateur ça représente une cellule noire.

Je vais donc chercher ces valeurs "Opti" :
`SELECT elem -> :caracteristic -» :caracteristicOpti AS caracteristic_opti`
ainsi que la vitesse :  `input_vitesse -» 'inputVitesse'              AS input_vitesse`

Cela me retourne un tableau :

```json
{
  "caracteristic_opti": "30",
  "date_releve": "07/03/2025 15:05:30",
  "nom_pn_plating": "77310-159LF",
  "initiale_user": "LAMA",
  "initiale_userat": "",
  "input_vitesse": "6.7"
}
```

que je peux donc exploiter pour mon graphique en récupérant donc la date et la valeur pour constituer mes axes.

Afin d'établir mes limites je réalise aussi une requête qui va me récupérer les moyennes hautes et basse.

Même principe qu’auparavant, mais je calcule la moyenne, qui va me permetre d'appliquer mes limites dans le
graphique. La requête va simplement me retourner la moyenne des valeurs relevées pour la caractéristique sélectionnée
pour un produit et une étape donnée dans l'objet JSON stocké en base de données.

In fine nous avons un graphique avec ApexChart sur l'évolution des caractéristiques
sélectionnées pour un ou plusieurs produits.

Ce graphique ne sert pas à faire beau, il est très utile afin de voir si il n'y a pas d'anomalies de production sur
l'ensemble des relevés, et si anomalie il y a, alors aviser la dite caractéristique pour par exemple élargir la plage
minimale et maximale ou bien la diminuer.

J'ai mis en oeuvre le développement au sein d'une application existante, donc l'ajout de fonctionnalité,
développer la sélection et l'envoi de données hétérogènes via une api afin de les exploiter de manière statistique.

# Bilan

Le développement de ce dashboard a permis de répondre à un besoin de l'entreprise en matière de suivi de la qualité des
produits. J'ai pu mettre en place une solution technique à l'aide de ce que j'ai appris en cours à l'IUT. À savoir
interroger une base de données, récupérer des données, les traiter et les afficher de manière graphique tout cela en
respectant les bonnes pratiques d'architecture logicielle.

J'ai su à partir d'un besoin métier, identifier les problèmes liés à la gestion des relevés de caractéristiques et
proposer une solution adaptée et faire évoluer l'application existante dans un environnement de production en
choisissant les architectures adaptées. Le tout en respectant les bonnes pratiques de développement.

Cette expérience m'a permis de renforcer mes compétences en développement web, en gestion de base de données et en
sécurité des applications.

Ce dashboard remplit donc son rôle de suivi des caractéristiques de production, permettant aux équipes de détecter les
dérives et d'agir en conséquence.